# Copia este archivo como .env (NO lo subas al repo)

# Puerto interno del servidor (en Docker normalmente se deja en 3000)
PORT=3000

# Requerido para Telegram
TELEGRAM_BOT_TOKEN=

# OpenAI (recomendado para respuestas reales)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# URL pública para registrar el webhook de Telegram (opcional en local)
WEBHOOK_BASE=

# Embeddings / chunking (opcionales)
EMBEDDING_CHUNK_SIZE=1400
EMBEDDING_CHUNK_OVERLAP=200
EMBEDDING_MAX_CHUNKS=600

# Corta el texto extraído guardado por documento (útil para PDFs enormes)
# Nota: aunque el PDF tenga más texto, el sistema solo guardará hasta este número de caracteres.
DOCUMENT_MAX_EXTRACTED_CHARS=2000000

# Para no saturar la UI, el backend trunca el texto extraído enviado a /api/documents.
CLIENT_EXTRACTED_TEXT_LIMIT=20000

# Límites de documentos (opcionales)
# Si quieres que SIEMPRE se incluyan TODOS los documentos cargados en el prompt,
# pon esto en true (ojo: puede aumentar costos y hacer que el prompt sea muy largo).
INCLUDE_ALL_DOCUMENTS=false

# Para intentar que el bot “lea completo” cada documento, sube estos límites.
# Importante: si el total excede la ventana de contexto del modelo, OpenAI puede
# rechazar la petición o la respuesta puede degradarse.
DOCUMENT_PER_DOC_LIMIT=120000
DOCUMENT_PER_DOC_LIMIT_ALL=120000
DOCUMENT_TOTAL_LIMIT=600000
DOCUMENT_TOTAL_LIMIT_ALL=1200000

# Límite usado cuando el bot decide devolver un documento completo (caso “contenido completo”).
DOCUMENT_FULL_LIMIT=

# OCR (el contenedor liviano NO incluye tesseract/poppler; solo aplica si los instalas)
TESSERACT_LANG=spa
